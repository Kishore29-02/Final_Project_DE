{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA ENGINEERING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached psycopg2-2.9.9-cp312-cp312-win_amd64.whl.metadata (4.5 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting openpyxl\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Using cached numpy-2.1.2-cp312-cp312-win_amd64.whl.metadata (59 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kishore\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kishore\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kishore\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Using cached et_xmlfile-1.1.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kishore\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached psycopg2-2.9.9-cp312-cp312-win_amd64.whl (1.2 MB)\n",
      "Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached numpy-2.1.2-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "Using cached et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: python-dotenv, psycopg2, numpy, et-xmlfile, pandas, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 numpy-2.1.2 openpyxl-3.1.5 pandas-2.2.3 psycopg2-2.9.9 python-dotenv-1.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script dotenv.exe is installed in 'c:\\Users\\Kishore\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts f2py.exe and numpy-config.exe are installed in 'c:\\Users\\Kishore\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "%pip install psycopg2 pandas openpyxl python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAW LAYER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the tables from the source DB and store it in raw layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./1.Raw\\_prisma_migrations.csv\n",
      "Saved: ./1.Raw\\Employees.csv\n",
      "Saved: ./1.Raw\\Courses.csv\n",
      "Saved: ./1.Raw\\Designation_Courses.csv\n",
      "Saved: ./1.Raw\\Course_Performances.csv\n",
      "Saved: ./1.Raw\\Project_Performances.csv\n",
      "Saved: ./1.Raw\\Resignation_Records.csv\n",
      "Saved: ./1.Raw\\User_Accounts.csv\n",
      "Data has been saved to ./1.Raw as separate CSV files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kishore\\AppData\\Local\\Temp\\ipykernel_35596\\3161739222.py:51: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n",
      "C:\\Users\\Kishore\\AppData\\Local\\Temp\\ipykernel_35596\\3161739222.py:51: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n",
      "C:\\Users\\Kishore\\AppData\\Local\\Temp\\ipykernel_35596\\3161739222.py:51: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n",
      "C:\\Users\\Kishore\\AppData\\Local\\Temp\\ipykernel_35596\\3161739222.py:51: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n",
      "C:\\Users\\Kishore\\AppData\\Local\\Temp\\ipykernel_35596\\3161739222.py:51: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n",
      "C:\\Users\\Kishore\\AppData\\Local\\Temp\\ipykernel_35596\\3161739222.py:51: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n",
      "C:\\Users\\Kishore\\AppData\\Local\\Temp\\ipykernel_35596\\3161739222.py:51: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n",
      "C:\\Users\\Kishore\\AppData\\Local\\Temp\\ipykernel_35596\\3161739222.py:51: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Fetching the credentials from the environment variables\n",
    "dbname = os.getenv('DB_NAME')\n",
    "user = os.getenv('DB_USER')\n",
    "password = os.getenv('DB_PASSWORD')\n",
    "host = os.getenv('DB_HOST')\n",
    "port = os.getenv('DB_PORT')\n",
    "\n",
    "try:\n",
    "    # Connect to your PostgreSQL database\n",
    "    connection = psycopg2.connect(\n",
    "        dbname=dbname,\n",
    "        user=user,\n",
    "        password=password,\n",
    "        host=host,\n",
    "        port=port\n",
    "    )\n",
    "\n",
    "    # Create a cursor object\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT table_name\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema = 'public';\n",
    "    \"\"\")\n",
    "\n",
    "    tables = cursor.fetchall()\n",
    "\n",
    "    output_dir = './1.Raw'  # Adjusted output directory to 'data'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for table in tables:\n",
    "        table_name = table[0]\n",
    "        if table_name == 'User_Accounts':\n",
    "            query = f'SELECT employee_id,email,role FROM public.\"{table_name}\"'\n",
    "        else:\n",
    "            query = f'SELECT * FROM public.\"{table_name}\"'\n",
    "        df = pd.read_sql(query, connection)\n",
    "\n",
    "        df = df.astype(str)  # Convert all columns to string\n",
    "\n",
    "        # Save each table as a separate CSV file\n",
    "        csv_file = os.path.join(output_dir, f\"{table_name}.csv\")\n",
    "        df.to_csv(csv_file, index=False)  # Saving as CSV\n",
    "        print(f\"Saved: {csv_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the cursor and connection\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if connection:\n",
    "        connection.close()\n",
    "\n",
    "print(f\"Data has been saved to {output_dir} as separate CSV files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Staging Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Data from Raw layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "input_dir = './1.Raw'\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "course_performances = pd.read_csv(f'{input_dir}/Course_Performances.csv')\n",
    "courses = pd.read_csv(f'{input_dir}/Courses.csv')\n",
    "designation_courses = pd.read_csv(f'{input_dir}/Designation_Courses.csv')\n",
    "employees = pd.read_csv(f'{input_dir}/Employees.csv')\n",
    "project_performance = pd.read_csv(f'{input_dir}/Project_Performances.csv')\n",
    "resignation_records = pd.read_csv(f'{input_dir}/Resignation_Records.csv')\n",
    "user_account = pd.read_csv(f'{input_dir}/User_Accounts.csv')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "input_dir = './1.Raw'\n",
    "\n",
    "# Load CSV files into DataFrames\n",
    "course_performances = pd.read_csv(f'{input_dir}/Course_Performances.csv')\n",
    "courses = pd.read_csv(f'{input_dir}/Courses.csv')\n",
    "designation_courses = pd.read_csv(f'{input_dir}/Designation_Courses.csv')\n",
    "employees = pd.read_csv(f'{input_dir}/Employees.csv')\n",
    "project_performance = pd.read_csv(f'{input_dir}/Project_Performances.csv')\n",
    "resignation_records = pd.read_csv(f'{input_dir}/Resignation_Records.csv')\n",
    "user_account = pd.read_csv(f'{input_dir}/User_Accounts.csv')   \n",
    "\n",
    "# Function to perform basic data cleaning\n",
    "def clean_dataframe(df):\n",
    "    # Check for missing values\n",
    "    print(\"Missing values before cleaning:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Rename columns (optional, based on your needs)\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "\n",
    "    # Trim whitespaces from string columns\n",
    "    string_cols = df.select_dtypes(include=[object]).columns\n",
    "    df[string_cols] = df[string_cols].apply(lambda x: x.str.strip())\n",
    "\n",
    "    # Check for missing values after cleaning\n",
    "    print(\"Missing values after cleaning:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Clean each DataFrame\n",
    "course_performances_stage = clean_dataframe(course_performances)\n",
    "courses_stage = clean_dataframe(courses)\n",
    "designation_courses_stage = clean_dataframe(designation_courses)\n",
    "employees_stage = clean_dataframe(employees)\n",
    "project_performance_stage = clean_dataframe(project_performance)\n",
    "resignation_records_stage = clean_dataframe(resignation_records)\n",
    "user_account_stage = clean_dataframe(user_account)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Course Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                          int64\n",
      "employee_id                 int64\n",
      "course_id                   int64\n",
      "course_status              object\n",
      "score                       int64\n",
      "completion_date    datetime64[ns]\n",
      "dtype: object\n",
      "Cleaned DataFrame has been saved to CSV.\n"
     ]
    }
   ],
   "source": [
    "course_performances_stage['score'] = course_performances_stage['score'].astype(int)\n",
    "course_performances_stage['completion_date'] = pd.to_datetime(course_performances_stage['completion_date'], errors='coerce')\n",
    "\n",
    "print(course_performances_stage.dtypes)\n",
    "\n",
    "output_dir = './2.Staging'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "course_performances_stage.to_csv(f'{output_dir}/Course_Performances.csv', index=False)\n",
    "\n",
    "print(\"Course Performances has been saved to CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                     int64\n",
      "course_name           object\n",
      "course_description    object\n",
      "duration_hours         int64\n",
      "dtype: object\n",
      "Cleaned DataFrame has been saved to CSV.\n"
     ]
    }
   ],
   "source": [
    "courses_stage['duration_hours'] = courses_stage['duration_hours'].astype(int)\n",
    "# courses_stage['completion_date'] = pd.to_datetime(courses_stage['completion_date'], errors='coerce')\n",
    "\n",
    "print(courses_stage.dtypes)\n",
    "\n",
    "output_dir = './2.Staging'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "courses_stage.to_csv(f'{output_dir}/Courses.csv', index=False)\n",
    "\n",
    "print(\"Cleaned DataFrame has been saved to CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Designation Courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "designation_type    object\n",
      "course_id            int64\n",
      "dtype: object\n",
      "Cleaned DataFrame has been saved to CSV.\n"
     ]
    }
   ],
   "source": [
    "designation_courses_stage['course_id'] = designation_courses_stage['course_id'].astype(int)\n",
    "\n",
    "print(designation_courses_stage.dtypes)\n",
    "\n",
    "output_dir = './2.Staging'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "designation_courses_stage.to_csv(f'{output_dir}/Designation_Courses.csv', index=False)\n",
    "\n",
    "print(\"Cleaned DataFrame has been saved to CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                            int64\n",
      "first_name                   object\n",
      "last_name                    object\n",
      "department                   object\n",
      "designation_type             object\n",
      "hire_date            datetime64[ns]\n",
      "employment_status            object\n",
      "dtype: object\n",
      "Course Performances has been saved to CSV.\n"
     ]
    }
   ],
   "source": [
    "employees_stage['id'] = employees_stage['id'].astype(int)\n",
    "employees_stage['hire_date'] = pd.to_datetime(employees_stage['hire_date'], errors='coerce')\n",
    "\n",
    "print(employees_stage.dtypes)\n",
    "\n",
    "output_dir = './2.Staging'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "employees_stage.to_csv(f'{output_dir}/Employees.csv', index=False)\n",
    "\n",
    "print(\"Course Performances has been saved to CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Project Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                           int64\n",
      "employee_id                  int64\n",
      "project_id                   int64\n",
      "engagement_score             int64\n",
      "teamwork_score               int64\n",
      "punctuality_score            int64\n",
      "overall_performance_score    int64\n",
      "dtype: object\n",
      "Course Performances has been saved to CSV.\n"
     ]
    }
   ],
   "source": [
    "project_performance_stage = project_performance_stage.astype(int) \n",
    "\n",
    "print(project_performance_stage.dtypes)\n",
    "\n",
    "output_dir = './2.Staging'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "project_performance_stage.to_csv(f'{output_dir}/Project_Performances.csv', index=False)\n",
    "\n",
    "print(\"Course Performances has been saved to CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Resignation Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                           int64\n",
      "employee_id                  int64\n",
      "resignation_date    datetime64[ns]\n",
      "reason                      object\n",
      "dtype: object\n",
      "Course Performances has been saved to CSV.\n"
     ]
    }
   ],
   "source": [
    "resignation_records_stage['id'] = resignation_records_stage['id'].astype(int)\n",
    "resignation_records_stage['employee_id'] = resignation_records_stage['employee_id'].astype(int)\n",
    "resignation_records_stage['resignation_date'] = pd.to_datetime(resignation_records_stage['resignation_date'], errors='coerce')\n",
    "\n",
    "print(resignation_records_stage.dtypes)\n",
    "\n",
    "output_dir = './2.Staging'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "resignation_records_stage.to_csv(f'{output_dir}/Resignation_Records.csv', index=False)\n",
    "\n",
    "print(\"Course Performances has been saved to CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning User Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_performances_stage['score'] = course_performances_stage['score'].astype(int)\n",
    "course_performances_stage['completion_date'] = pd.to_datetime(course_performances_stage['completion_date'], errors='coerce')\n",
    "\n",
    "print(course_performances_stage.dtypes)\n",
    "\n",
    "output_dir = './2.Staging'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "course_performances_stage.to_csv(f'{output_dir}/Course_Performances.csv', index=False)\n",
    "\n",
    "print(\"Course Performances has been saved to CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 402 entries, 0 to 401\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   id               402 non-null    int64 \n",
      " 1   employee_id      402 non-null    int64 \n",
      " 2   course_id        402 non-null    int64 \n",
      " 3   course_status    402 non-null    object\n",
      " 4   score            402 non-null    int64 \n",
      " 5   completion_date  402 non-null    object\n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 19.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 402 entries, 0 to 401\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   id               402 non-null    int64         \n",
      " 1   employee_id      402 non-null    int64         \n",
      " 2   course_id        402 non-null    int64         \n",
      " 3   course_status    402 non-null    object        \n",
      " 4   score            402 non-null    int64         \n",
      " 5   completion_date  402 non-null    datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(4), object(1)\n",
      "memory usage: 19.0+ KB\n"
     ]
    }
   ],
   "source": [
    "temp = pd.read_csv(f'./2.Staging/Course_Performances.csv')\n",
    "\n",
    "temp.info()\n",
    "course_performances_stage.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
