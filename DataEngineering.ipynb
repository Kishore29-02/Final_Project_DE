{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA ENGINEERING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2 in c:\\users\\kishore\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.9.9)\n",
      "Requirement already satisfied: pandas in c:\\users\\kishore\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\kishore\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\kishore\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: logging in c:\\users\\kishore\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.4.9.6)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\kishore\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kishore\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kishore\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kishore\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\kishore\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kishore\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install psycopg2 pandas openpyxl python-dotenv logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    filename='elt_process.log',  # Log file\n",
    "    level=logging.DEBUG,          # Log level\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'  # Log format\n",
    ")\n",
    "\n",
    "# Create a logger\n",
    "logger = logging.getLogger()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAW LAYER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the tables from the source DB and store it in raw layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./1.Raw\\_prisma_migrations.csv\n",
      "Saved: ./1.Raw\\Employees.csv\n",
      "Saved: ./1.Raw\\Courses.csv\n",
      "Saved: ./1.Raw\\Designation_Courses.csv\n",
      "Saved: ./1.Raw\\Course_Performances.csv\n",
      "Saved: ./1.Raw\\Project_Performances.csv\n",
      "Saved: ./1.Raw\\Resignation_Records.csv\n",
      "Saved: ./1.Raw\\User_Accounts.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kishore\\AppData\\Local\\Temp\\ipykernel_9576\\2718768494.py:46: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n",
      "C:\\Users\\Kishore\\AppData\\Local\\Temp\\ipykernel_9576\\2718768494.py:46: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n",
      "C:\\Users\\Kishore\\AppData\\Local\\Temp\\ipykernel_9576\\2718768494.py:46: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n",
      "C:\\Users\\Kishore\\AppData\\Local\\Temp\\ipykernel_9576\\2718768494.py:46: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n",
      "C:\\Users\\Kishore\\AppData\\Local\\Temp\\ipykernel_9576\\2718768494.py:46: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n",
      "C:\\Users\\Kishore\\AppData\\Local\\Temp\\ipykernel_9576\\2718768494.py:46: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n",
      "C:\\Users\\Kishore\\AppData\\Local\\Temp\\ipykernel_9576\\2718768494.py:46: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n",
      "C:\\Users\\Kishore\\AppData\\Local\\Temp\\ipykernel_9576\\2718768494.py:46: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Fetching the credentials from the environment variables\n",
    "dbname = os.getenv('DB_NAME')\n",
    "user = os.getenv('DB_USER')\n",
    "password = os.getenv('DB_PASSWORD')\n",
    "host = os.getenv('DB_HOST')\n",
    "port = os.getenv('DB_PORT')\n",
    "\n",
    "try:\n",
    "    # Connect to your PostgreSQL database\n",
    "    connection = psycopg2.connect(\n",
    "        dbname=dbname,\n",
    "        user=user,\n",
    "        password=password,\n",
    "        host=host,\n",
    "        port=port\n",
    "    )\n",
    "\n",
    "    # Create a cursor object\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT table_name\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema = 'public';\n",
    "    \"\"\")\n",
    "\n",
    "    tables = cursor.fetchall()\n",
    "\n",
    "    output_dir = './1.Raw'  # Adjusted output directory to 'data'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for table in tables:\n",
    "        table_name = table[0]\n",
    "        if table_name == 'User_Accounts':\n",
    "            query = f'SELECT employee_id,role FROM public.\"{table_name}\"'\n",
    "        else:\n",
    "            query = f'SELECT * FROM public.\"{table_name}\"'\n",
    "        df = pd.read_sql(query, connection)\n",
    "\n",
    "        df = df.astype(str)  # Convert all columns to string\n",
    "\n",
    "        # Save each table as a separate CSV file\n",
    "        csv_file = os.path.join(output_dir, f\"{table_name}.csv\")\n",
    "        df.to_csv(csv_file, index=False)  # Saving as CSV\n",
    "        print(f\"Saved: {csv_file}\")\n",
    "\n",
    "    logger.info(f\"Data has been saved to {output_dir} as separate CSV files.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    logger.error(\"Unable to load data into raw layer: %s\", e)\n",
    "\n",
    "finally:\n",
    "    # Close the cursor and connection\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if connection:\n",
    "        connection.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Data from Raw layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "input_dir = './1.Raw'\n",
    "\n",
    "try:\n",
    "    course_performances = pd.read_csv(f'{input_dir}/Course_Performances.csv')\n",
    "    courses = pd.read_csv(f'{input_dir}/Courses.csv')\n",
    "    designation_courses = pd.read_csv(f'{input_dir}/Designation_Courses.csv')\n",
    "    employees = pd.read_csv(f'{input_dir}/Employees.csv')\n",
    "    project_performance = pd.read_csv(f'{input_dir}/Project_Performances.csv')\n",
    "    resignation_records = pd.read_csv(f'{input_dir}/Resignation_Records.csv')\n",
    "    user_account = pd.read_csv(f'{input_dir}/User_Accounts.csv')  \n",
    "    logger.info(\"Data has been read from Raw into Staging\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    logger.error(\"Error Reading data from Raw: %s\", e)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    filename='elt_process.log',  # Log file\n",
    "    level=logging.DEBUG,          # Log level\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'  # Log format\n",
    ")\n",
    "\n",
    "# Create a logger\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def clean_dataframe(df, df_name):\n",
    "    logger.info(f\"Cleaning DataFrame: {df_name}\")\n",
    "\n",
    "    # Check for missing values\n",
    "    logger.info(\"Missing values before cleaning:\")\n",
    "    missing_before = df.isnull().sum()\n",
    "    logger.info(f\"{missing_before}\")\n",
    "\n",
    "    # Remove duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    logger.info(\"Duplicates removed.\")\n",
    "\n",
    "    # Rename columns (optional, based on your needs)\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "    logger.info(\"Columns renamed to lower case and spaces replaced with underscores.\")\n",
    "\n",
    "    # Trim whitespaces from string columns\n",
    "    string_cols = df.select_dtypes(include=[object]).columns\n",
    "    df[string_cols] = df[string_cols].apply(lambda x: x.str.strip())\n",
    "    logger.info(\"Whitespace trimmed from string columns.\")\n",
    "\n",
    "    # Check for missing values after cleaning\n",
    "    logger.info(\"Missing values after cleaning:\")\n",
    "    missing_after = df.isnull().sum()\n",
    "    logger.info(f\"{missing_after}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Clean each DataFrame with logging\n",
    "course_performances_stage = clean_dataframe(course_performances, \"course_performances\")\n",
    "courses_stage = clean_dataframe(courses, \"courses\")\n",
    "designation_courses_stage = clean_dataframe(designation_courses, \"designation_courses\")\n",
    "employees_stage = clean_dataframe(employees, \"employees\")\n",
    "project_performance_stage = clean_dataframe(project_performance, \"project_performance\")\n",
    "resignation_records_stage = clean_dataframe(resignation_records, \"resignation_records\")\n",
    "user_account_stage = clean_dataframe(user_account, \"user_account\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Course Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Convert 'score' column to integer\n",
    "    course_performances_stage['score'] = course_performances_stage['score'].astype(int)\n",
    "    logger.info(\"Converted 'score' column to integer type.\")\n",
    "\n",
    "    # Convert 'completion_date' column to datetime\n",
    "    course_performances_stage['completion_date'] = pd.to_datetime(course_performances_stage['completion_date'], errors='coerce')\n",
    "    logger.info(\"Converted 'completion_date' column to datetime type.\")\n",
    "\n",
    "    # Log the data types\n",
    "    dtypes_info = course_performances_stage.dtypes\n",
    "    logger.info(f\"Data types after conversion:\\n{dtypes_info}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during type conversion: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Convert 'duration_hours' column to integer\n",
    "    courses_stage['duration_hours'] = courses_stage['duration_hours'].astype(int)\n",
    "    logger.info(\"Converted 'duration_hours' column to integer type.\")\n",
    "\n",
    "    # Log the data types\n",
    "    dtypes_info = courses_stage.dtypes\n",
    "    logger.info(f\"Data types after conversion:\\n{dtypes_info}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during type conversion: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Designation Courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Convert 'course_id' column to integer\n",
    "    designation_courses_stage['course_id'] = designation_courses_stage['course_id'].astype(int)\n",
    "    logger.info(\"Converted 'course_id' column to integer type.\")\n",
    "\n",
    "    # Log the data types\n",
    "    dtypes_info = designation_courses_stage.dtypes\n",
    "    logger.info(f\"Data types after conversion:\\n{dtypes_info}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during type conversion: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Employees and User Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Clean employees_stage\n",
    "    employees_stage['id'] = employees_stage['id'].astype(int)\n",
    "    logger.info(\"Converted 'id' column in employees_stage to integer type.\")\n",
    "\n",
    "    employees_stage['hire_date'] = pd.to_datetime(employees_stage['hire_date'], errors='coerce')\n",
    "    logger.info(\"Converted 'hire_date' column in employees_stage to datetime type.\")\n",
    "\n",
    "    # Clean user_account_stage\n",
    "    user_account_stage['employee_id'] = user_account_stage['employee_id'].astype(int)\n",
    "    logger.info(\"Converted 'employee_id' column in user_account_stage to integer type.\")\n",
    "\n",
    "    # Merge the tables\n",
    "    employee_details_stage = pd.merge(employees_stage, user_account_stage, left_on=\"id\", right_on=\"employee_id\", how=\"inner\")\n",
    "    logger.info(\"Merged employees_stage and user_account_stage.\")\n",
    "\n",
    "    employee_details_stage = employee_details_stage.drop(columns=['employee_id'])\n",
    "    logger.info(\"Dropped 'employee_id' column from employee_details_stage.\")\n",
    "\n",
    "    # Log the data types of the resulting DataFrame\n",
    "    dtypes_info = employee_details_stage.dtypes\n",
    "    logger.info(f\"Data types in employee_details_stage:\\n{dtypes_info}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during data cleaning and merging: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Project Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Convert all columns to integer type\n",
    "    project_performance_stage = project_performance_stage.astype(int)\n",
    "    logger.info(\"Converted all columns in project_performance_stage to integer type.\")\n",
    "\n",
    "    # Log the data types of the resulting DataFrame\n",
    "    dtypes_info = project_performance_stage.dtypes\n",
    "    logger.info(f\"Data types in project_performance_stage:\\n{dtypes_info}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during type conversion in project_performance_stage: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Resignation Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Convert 'id' column to integer\n",
    "    resignation_records_stage['id'] = resignation_records_stage['id'].astype(int)\n",
    "    logger.info(\"Converted 'id' column in resignation_records_stage to integer type.\")\n",
    "\n",
    "    # Convert 'employee_id' column to integer\n",
    "    resignation_records_stage['employee_id'] = resignation_records_stage['employee_id'].astype(int)\n",
    "    logger.info(\"Converted 'employee_id' column in resignation_records_stage to integer type.\")\n",
    "\n",
    "    # Convert 'resignation_date' column to datetime\n",
    "    resignation_records_stage['resignation_date'] = pd.to_datetime(resignation_records_stage['resignation_date'], errors='coerce')\n",
    "    logger.info(\"Converted 'resignation_date' column in resignation_records_stage to datetime type.\")\n",
    "\n",
    "    # Log the data types of the resulting DataFrame\n",
    "    dtypes_info = resignation_records_stage.dtypes\n",
    "    logger.info(f\"Data types in resignation_records_stage:\\n{dtypes_info}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during data cleaning in resignation_records_stage: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staging has been saved to CSV.\n"
     ]
    }
   ],
   "source": [
    "output_dir = './2.Staging' \n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    course_performances_stage.to_csv(f'{output_dir}/Course_Performances.csv', index=False)\n",
    "    logger.info(\"Saved Course_Performances.csv\")\n",
    "\n",
    "    courses_stage.to_csv(f'{output_dir}/Courses.csv', index=False)\n",
    "    logger.info(\"Saved Courses.csv\")\n",
    "\n",
    "    designation_courses_stage.to_csv(f'{output_dir}/Designation_Courses.csv', index=False)\n",
    "    logger.info(\"Saved Designation_Courses.csv\")\n",
    "\n",
    "    employees_stage.to_csv(f'{output_dir}/Employees_Details.csv', index=False)\n",
    "    logger.info(\"Saved Employees_Details.csv\")\n",
    "\n",
    "    project_performance_stage.to_csv(f'{output_dir}/Project_Performances.csv', index=False)\n",
    "    logger.info(\"Saved Project_Performances.csv\")\n",
    "\n",
    "    resignation_records_stage.to_csv(f'{output_dir}/Resignation_Records.csv', index=False)\n",
    "    logger.info(\"Saved Resignation_Records.csv\")\n",
    "\n",
    "    user_account_stage.to_csv(f'{output_dir}/User_Account.csv', index=False)\n",
    "    logger.info(\"Saved User_Account.csv\")\n",
    "\n",
    "    print(\"Staging has been saved to CSV.\")\n",
    "    logger.info(\"Staging has been saved to CSV.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error saving DataFrames to CSV: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data from Perp Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# input_dir = './2.Prep'\n",
    "\n",
    "# dataframes = []\n",
    "\n",
    "# course_performances_prep = pd.read_csv(f'{input_dir}/Course_Performances.csv')\n",
    "# courses_prep = pd.read_csv(f'{input_dir}/Courses.csv')\n",
    "# designation_courses_prep = pd.read_csv(f'{input_dir}/Designation_Courses.csv')\n",
    "# employees_prep = pd.read_csv(f'{input_dir}/Employees.csv')\n",
    "# project_performance_prep = pd.read_csv(f'{input_dir}/Project_Performances.csv')\n",
    "# resignation_records_prep = pd.read_csv(f'{input_dir}/Resignation_Records.csv')\n",
    "# user_account_prep = pd.read_csv(f'{input_dir}/User_Accounts.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Prepare DataFrames\n",
    "    course_performances_prep = course_performances_stage\n",
    "    logger.info(\"Prepared course_performances_prep DataFrame.\")\n",
    "\n",
    "    courses_prep = courses_stage\n",
    "    logger.info(\"Prepared courses_prep DataFrame.\")\n",
    "\n",
    "    designation_courses_prep = designation_courses_stage\n",
    "    logger.info(\"Prepared designation_courses_prep DataFrame.\")\n",
    "\n",
    "    employees_prep = employee_details_stage\n",
    "    logger.info(\"Prepared employees_prep DataFrame.\")\n",
    "\n",
    "    project_performance_prep = project_performance_stage\n",
    "    logger.info(\"Prepared project_performance_prep DataFrame.\")\n",
    "\n",
    "    resignation_records_prep = resignation_records_stage\n",
    "    logger.info(\"Prepared resignation_records_prep DataFrame.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during DataFrame preparation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fact Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   employee_id  course_id course_status  score completion_date\n",
      "0            1          6     completed     74      2024-03-02\n",
      "1            1         10    incomplete      0      2020-09-22\n",
      "2            1          8        failed      0      2023-06-14\n",
      "3            1          9        failed      0      2019-12-10\n",
      "4            2          4        failed      0      2023-12-15\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Drop the 'id' column\n",
    "    course_performances_prep = course_performances_prep.drop(columns=['id'])\n",
    "    logger.info(\"Dropped 'id' column from course_performances_prep DataFrame.\")\n",
    "\n",
    "    # Display the first few rows of the DataFrame\n",
    "    logger.info(\"Displaying first few rows of course_performances_prep:\")\n",
    "    print(course_performances_prep.head())\n",
    "\n",
    "except KeyError as e:\n",
    "    logger.error(f\"Error: The specified column was not found in course_performances_prep - {e}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during operation on course_performances_prep: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   employee_id  project_id  engagement_score  teamwork_score  \\\n",
      "0            1           1                96              42   \n",
      "1            1           2                67              37   \n",
      "2            1           3                60              92   \n",
      "3            2           1                90              21   \n",
      "4            2           2                92              30   \n",
      "\n",
      "   punctuality_score  overall_performance_score  \n",
      "0                 89                         23  \n",
      "1                 52                         36  \n",
      "2                 30                         10  \n",
      "3                 10                         19  \n",
      "4                 62                         96  \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Drop the 'id' column\n",
    "    project_performance_prep = project_performance_prep.drop(columns=['id'])\n",
    "    logger.info(\"Dropped 'id' column from project_performance_prep DataFrame.\")\n",
    "\n",
    "    # Display the first few rows of the DataFrame\n",
    "    logger.info(\"Displaying first few rows of project_performance_prep:\")\n",
    "    print(project_performance_prep.head())\n",
    "\n",
    "except KeyError as e:\n",
    "    logger.error(f\"Error: The specified column was not found in project_performance_prep - {e}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during operation on project_performance_prep: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resignation Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   employee_id resignation_date  \\\n",
      "0           35       2021-06-04   \n",
      "1           80       2024-06-20   \n",
      "2           62       2024-08-25   \n",
      "3           51       2024-07-23   \n",
      "4           27       2024-08-06   \n",
      "\n",
      "                                              reason  \n",
      "0                  Any law entire. School half kind.  \n",
      "1  Range table matter discover follow different box.  \n",
      "2             Structure act expert require approach.  \n",
      "3                                   About away here.  \n",
      "4              She dinner whose like budget certain.  \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Drop the 'id' column\n",
    "    resignation_records_prep = resignation_records_prep.drop(columns=['id'])\n",
    "    logger.info(\"Dropped 'id' column from resignation_records_prep DataFrame.\")\n",
    "\n",
    "    # Display the first few rows of the DataFrame\n",
    "    logger.info(\"Displaying first few rows of resignation_records_prep:\")\n",
    "    print(resignation_records_prep.head())\n",
    "\n",
    "except KeyError as e:\n",
    "    logger.error(f\"Error: The specified column was not found in resignation_records_prep - {e}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during operation on resignation_records_prep: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Employee Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   employee_id        100 non-null    int64         \n",
      " 1   first_name         100 non-null    object        \n",
      " 2   last_name          100 non-null    object        \n",
      " 3   department         100 non-null    object        \n",
      " 4   designation_type   100 non-null    object        \n",
      " 5   hire_date          100 non-null    datetime64[ns]\n",
      " 6   employment_status  100 non-null    object        \n",
      "dtypes: datetime64[ns](1), int64(1), object(5)\n",
      "memory usage: 5.6+ KB\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Drop the 'role' column\n",
    "    employees_prep = employees_prep.drop(columns=['role'])\n",
    "    logger.info(\"Dropped 'role' column from employees_prep DataFrame.\")\n",
    "\n",
    "    # Rename the 'id' column to 'employee_id'\n",
    "    employees_prep.rename(columns={'id': 'employee_id'}, inplace=True)\n",
    "    logger.info(\"Renamed 'id' column to 'employee_id' in employees_prep DataFrame.\")\n",
    "\n",
    "    # Display DataFrame information\n",
    "    logger.info(\"Displaying information of employees_prep DataFrame:\")\n",
    "    employees_prep_info = employees_prep.info()  # This will print the info to console\n",
    "    logger.info(f\"DataFrame information:\\n{employees_prep_info}\")\n",
    "\n",
    "except KeyError as e:\n",
    "    logger.error(f\"Error: The specified column was not found in employees_prep - {e}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during operation on employees_prep: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16 entries, 0 to 15\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   course_id           16 non-null     int64 \n",
      " 1   course_name         16 non-null     object\n",
      " 2   course_description  16 non-null     object\n",
      " 3   duration_hours      16 non-null     int64 \n",
      " 4   designation_type    16 non-null     object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 772.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Merge DataFrames\n",
    "    courses_prep = pd.merge(courses_prep, designation_courses_prep, left_on='id', right_on='course_id')\n",
    "    logger.info(\"Merged courses_prep with designation_courses_prep DataFrame on 'id' and 'course_id'.\")\n",
    "\n",
    "    # Drop the 'course_id' column\n",
    "    courses_prep = courses_prep.drop(columns=['course_id'])\n",
    "    logger.info(\"Dropped 'course_id' column from courses_prep DataFrame.\")\n",
    "\n",
    "    # Rename the 'id' column to 'course_id'\n",
    "    courses_prep.rename(columns={'id': 'course_id'}, inplace=True)\n",
    "    logger.info(\"Renamed 'id' column to 'course_id' in courses_prep DataFrame.\")\n",
    "\n",
    "    # Display DataFrame information\n",
    "    logger.info(\"Displaying information of courses_prep DataFrame:\")\n",
    "    courses_prep_info = courses_prep.info()  # This will print the info to console\n",
    "    logger.info(f\"DataFrame information:\\n{courses_prep_info}\")\n",
    "\n",
    "except KeyError as e:\n",
    "    logger.error(f\"Error: The specified column was not found during the merge operation in courses_prep - {e}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during operation on courses_prep: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './3.Report'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Save cleaned DataFrames to CSV\n",
    "    course_performances_prep.to_csv(f'{output_dir}/fact_Course_Performances.csv', index=False)\n",
    "    logger.info(\"Saved cleaned course_performances_prep DataFrame to fact_Course_Performances.csv.\")\n",
    "    \n",
    "    courses_prep.to_csv(f'{output_dir}/dim_Courses.csv', index=False)\n",
    "    logger.info(\"Saved cleaned courses_prep DataFrame to dim_Courses.csv.\")\n",
    "    \n",
    "    employees_prep.to_csv(f'{output_dir}/dim_Employees_Details.csv', index=False)\n",
    "    logger.info(\"Saved cleaned employees_prep DataFrame to dim_Employees_Details.csv.\")\n",
    "    \n",
    "    project_performance_prep.to_csv(f'{output_dir}/fact_Project_Performances.csv', index=False)\n",
    "    logger.info(\"Saved cleaned project_performance_prep DataFrame to fact_Project_Performances.csv.\")\n",
    "    \n",
    "    resignation_records_prep.to_csv(f'{output_dir}/fact_Resignation_Records.csv', index=False)\n",
    "    logger.info(\"Saved cleaned resignation_records_prep DataFrame to fact_Resignation_Records.csv.\")\n",
    "\n",
    "    logger.info(\"Data Mart has been saved to CSV.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during saving Data Mart to CSV: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './4.Final_Report'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designation Count by Course and Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   course_id designation_type course_status  employees_count\n",
      "0          1                A     completed               11\n",
      "1          1                A        failed               10\n",
      "2          1                A    incomplete                2\n",
      "3          2                D     completed                9\n",
      "4          2                D        failed                9\n"
     ]
    }
   ],
   "source": [
    "def designation_count_by_course_and_status(course_performances_stage, employees_stage):\n",
    "    try:\n",
    "        logger.info(\"Starting the designation count by course and status.\")\n",
    "\n",
    "        # Merge DataFrames\n",
    "        employee_with_designation = course_performances_stage.merge(\n",
    "            employees_stage[['employee_id', 'designation_type']],\n",
    "            left_on='employee_id',\n",
    "            right_on='employee_id'\n",
    "        )\n",
    "        logger.info(\"Merged course_performances_stage with employees_stage to include designation types.\")\n",
    "\n",
    "        # Group by course_id, designation_type, and course_status\n",
    "        designation_count = employee_with_designation.groupby(\n",
    "            ['course_id', 'designation_type', 'course_status']\n",
    "        ).agg(employees_count=('employee_id', 'count')).reset_index()\n",
    "        logger.info(\"Grouped data to count employees by course and designation.\")\n",
    "\n",
    "        logger.info(\"Completed designation count by course and status.\")\n",
    "\n",
    "        return designation_count.sort_values(['course_id', 'designation_type'])\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in designation_count_by_course_and_status: {e}\")\n",
    "\n",
    "# Example usage with DataFrames\n",
    "designation_count = designation_count_by_course_and_status(course_performances_prep, employees_prep)\n",
    "\n",
    "designation_count.to_csv(f'{output_dir}/Designation_Count.csv', index=False)\n",
    "logger.info(\"Saved report to Designation_Count.csv.\")\n",
    "\n",
    "logger.info(\"Displaying first few rows of designation_count DataFrame:\")\n",
    "print(designation_count.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course Status Count by Course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   course_id course_name                                 course_description  \\\n",
      "0          1    Course 1  Pm major always man open speech seat. Camera i...   \n",
      "1          1    Course 1  Pm major always man open speech seat. Camera i...   \n",
      "2          1    Course 1  Pm major always man open speech seat. Camera i...   \n",
      "3          2    Course 2  Environment charge technology several data dre...   \n",
      "4          2    Course 2  Environment charge technology several data dre...   \n",
      "\n",
      "   duration_hours course_status  status_count  \n",
      "0              26     completed            11  \n",
      "1              26        failed            10  \n",
      "2              26    incomplete             2  \n",
      "3              15     completed             9  \n",
      "4              15        failed             9  \n"
     ]
    }
   ],
   "source": [
    "def course_status_count_by_course(course_performances_stage, courses_stage):\n",
    "    try:\n",
    "        logger.info(\"Starting the course status count by course.\")\n",
    "\n",
    "        # Merge DataFrames\n",
    "        course_count = course_performances_stage.merge(\n",
    "            courses_stage[['course_id', 'course_name', 'course_description', 'duration_hours']],\n",
    "            left_on='course_id',\n",
    "            right_on='course_id'\n",
    "        )\n",
    "        logger.info(\"Merged course_performances_stage with courses_stage to include course details.\")\n",
    "\n",
    "        # Group by course_id and course_status\n",
    "        course_status_count = course_count.groupby(\n",
    "            ['course_id', 'course_name', 'course_description', 'duration_hours', 'course_status']\n",
    "        ).agg(status_count=('course_status', 'count')).reset_index()\n",
    "        logger.info(\"Grouped data to count course status.\")\n",
    "\n",
    "        logger.info(\"Completed course status count by course.\")\n",
    "        return course_status_count.sort_values(['course_id', 'status_count'], ascending=[True, False])\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in course_status_count_by_course: {e}\")\n",
    "\n",
    "# Example usage with DataFrames\n",
    "course_status_count = course_status_count_by_course(course_performances_prep, courses_prep)\n",
    "\n",
    "course_status_count.to_csv(f'{output_dir}/Course_Completion_status.csv', index=False)\n",
    "logger.info(\"Saved report to Course_Completion_status.csv.\")\n",
    "\n",
    "logger.info(\"Displaying first few rows of course_status_count DataFrame:\")\n",
    "print(course_status_count.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Top Scorer and Average Score by Designation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    course_id course_name Department   Department_topper  top_score  avg_score\n",
      "0           5    Course 5          D          Lisa Moses         99  81.500000\n",
      "1          11   Course 11          A      Kristin Lowery         90  72.333333\n",
      "2           8    Course 8          C  Arthur Fitzpatrick        100  68.500000\n",
      "3           9    Course 9          C  Arthur Fitzpatrick         94  68.857143\n",
      "4          10   Course 10          C         Steven Hart         98  74.857143\n",
      "5           4    Course 4          D      Kathleen Myers         97  85.166667\n",
      "6           2    Course 2          D      Kathleen Myers        100  78.000000\n",
      "7           6    Course 6          C    Latoya Rodriguez         92  72.000000\n",
      "8           9    Course 9          B      Madison Hooper         97  80.500000\n",
      "9          11   Course 11          B      Brittany Smith        100  79.125000\n",
      "10          7    Course 7          B     Melinda Sanchez         95  74.666667\n",
      "11          8    Course 8          B        Lucas Morgan         98  75.454545\n",
      "12          1    Course 1          A       William Davis        100  80.363636\n",
      "13          6    Course 6          B       Alexa Stewart         86  70.076923\n",
      "14         12   Course 12          A  Christopher Austin         94  75.000000\n",
      "15          6    Course 6          A         Fred Powell         99  78.833333\n"
     ]
    }
   ],
   "source": [
    "def top_scorer_and_avg_by_designation(course_performances_stage, employees_stage, courses_stage):\n",
    "    try:\n",
    "        logger.info(\"Starting the calculation of top scorer and average by designation.\")\n",
    "\n",
    "        # Filter for completed courses\n",
    "        completed_courses = course_performances_stage[course_performances_stage['course_status'] == 'completed']\n",
    "        logger.info(f\"Filtered completed courses: {len(completed_courses)} records found.\")\n",
    "\n",
    "        # Merge to get designation types\n",
    "        employee_with_designation = completed_courses.merge(\n",
    "            employees_stage[['employee_id', 'designation_type']],\n",
    "            left_on='employee_id',\n",
    "            right_on='employee_id'\n",
    "        )\n",
    "        logger.info(\"Merged completed courses with employees to include designation types.\")\n",
    "\n",
    "        # Calculate average score\n",
    "        average_score = employee_with_designation.groupby(\n",
    "            ['course_id', 'designation_type']\n",
    "        ).agg(avg_score=('score', 'mean')).reset_index()\n",
    "        logger.info(\"Calculated average score by course and designation.\")\n",
    "\n",
    "        # Calculate row numbers by score descending\n",
    "        employee_with_scores = completed_courses.merge(\n",
    "            employees_stage[['employee_id', 'first_name', 'last_name', 'designation_type']],\n",
    "            left_on='employee_id',\n",
    "            right_on='employee_id'\n",
    "        )\n",
    "        employee_with_scores['row_number'] = employee_with_scores.groupby(\n",
    "            ['course_id', 'designation_type']\n",
    "        )['score'].rank(method='first', ascending=False)\n",
    "        logger.info(\"Assigned row numbers based on scores.\")\n",
    "\n",
    "        # Filter for top scores\n",
    "        top_score = employee_with_scores[employee_with_scores['row_number'] == 1]\n",
    "        logger.info(f\"Identified top scorers: {len(top_score)} records found.\")\n",
    "\n",
    "        # Merge top score with average score and course names\n",
    "        final_table = top_score.merge(\n",
    "            average_score[['course_id', 'designation_type', 'avg_score']],\n",
    "            on=['course_id', 'designation_type'],\n",
    "            how='left'\n",
    "        ).merge(\n",
    "            courses_stage[['course_id', 'course_name']],\n",
    "            on='course_id',\n",
    "            how='left'\n",
    "        )\n",
    "        logger.info(\"Merged top scorers with average scores and course names.\")\n",
    "\n",
    "        final_table['Department_topper'] = final_table['first_name'] + ' ' + final_table['last_name']\n",
    "\n",
    "        final_columns = ['course_id', 'course_name', 'designation_type', 'Department_topper', 'score', 'avg_score']\n",
    "        final_table = final_table.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "        logger.info(\"Completed calculation of top scorer and average by designation.\")\n",
    "        \n",
    "        return final_table[final_columns].rename(columns={'designation_type': 'Department', 'score': 'top_score'})\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in top_scorer_and_avg_by_designation: {e}\")\n",
    "\n",
    "top_scorer_avg = top_scorer_and_avg_by_designation(course_performances_prep, employees_prep, courses_prep)\n",
    "\n",
    "top_scorer_avg.to_csv(f'{output_dir}/Top_And_Average_Score.csv', index=False)\n",
    "logger.info(\"Saved report to Top_And_Average_Score.csv.\")\n",
    "\n",
    "logger.info(\"Displaying top scorer and average DataFrame:\")\n",
    "print(top_scorer_avg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
